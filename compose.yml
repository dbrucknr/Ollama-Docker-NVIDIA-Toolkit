volumes:
  shimmy-cache:
    driver: local
  # ollama_data:
  #   name: ollama_data
  cargo-target:
    name: cargo-target

services:
  # Shimmy (Rust - Runs at 5 MB)
  shimmy:
    image: ghcr.io/michael-a-kuykendall/shimmy:latest
    container_name: shimmy-server
    ports:
      - "11435:11435" # Shimmy server port
    command: shimmy serve --bind 0.0.0.0:11435
    volumes:
      - ./models:/models # Mount your models directory
      - shimmy-cache:/root/.cache # Persistent cache for downloads
    environment:
      - SHIMMY_BASE_GGUF=/models # Point to mounted models
      - SHIMMY_PORT=11435 # Server port
      - SHIMMY_HOST=0.0.0.0 # Listen on all interfaces
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia # GPU support (optional)
              count: all
              capabilities: [gpu]

  # Ollama (Golang - Runs at 680MB+)
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   # Requires NVIDIA-Toolkit install (Host System):
  #   # https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  # Development Target: docker compose --profile dev up --build
  dev:
    image: rust-ai-chatbot:dev
    container_name: rust-ai-chatbot-dev
    ports:
      - "127.0.0.1:8000:8000"
    build:
      context: .
      dockerfile: ./core/Dockerfile
      target: dev
    profiles: ["dev"]
    volumes:
      - ./core:/app
      - ./core/static:/app/static # frontend build mounted
      - cargo-target:/root/.cargo/target
    working_dir: /app
    depends_on:
      - shimmy
    environment:
      # Fix
      OPENAI_API_BASE_URL: http://shimmy-server:11434
      OPENAI_API_KEY: shimmy-service
    command: watchexec --restart --watch src --exts rs cargo run

  # Production Target: docker compose --profile prod up --build -d
  prod:
    image: rust-ai-chatbot:prod
    container_name: rust-ai-chatbot-prod
    build:
      context: .
      dockerfile: ./core/Dockerfile
      target: runtime
    profiles: ["prod"]
    volumes:
      - cargo-target:/root/.cargo/target
    depends_on:
      - shimmy
    environment:
      # Fix
      OLLAMA_API_BASE_URL: http://shimmy-server:11434
    command: cargo run
